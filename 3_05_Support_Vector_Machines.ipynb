{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteHum/Application-security/blob/main/3_05_Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFL4vOyV0LnL"
      },
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "## Overview\n",
        "\n",
        "This lab continues the discussion of Support Vector based prediction tools.  We will leverage some of the setup code that we wrote in the last notebook and then immediately explore additional kernel functions to attempt to improve the classification performance.\n",
        "\n",
        "## Goals\n",
        "\n",
        "In this lab we will:\n",
        "\n",
        "* Explore kernel methods other than *linear*\n",
        "* Attempt to improve our classification accuracy\n",
        "* Experiment with performing dimensionality reduction before attempting to train a SVM\n",
        "\n",
        " \n",
        "## Estimated Time: 30 minutes\n",
        "\n",
        "This lab is a continuation of the last lab.  With that in mind, we would like to load our dataset again and prepare our `x`, `y`, `x_test`, and `y_test` arrays back in place.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 5.1\n",
        "\n",
        "Please copy all required code from your last lab.  When the following cell finishes executing you should have successfully created a dataframe containing all of the data from the first 20 files used in the previous lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq90-5qN0LnO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_file_list(starting_directory=\"../data/Day 2/BackBlaze/data_Q4_2020/\"):\n",
        "    final_list = list()\n",
        "    files = os.listdir(starting_directory)\n",
        "    for file in files:\n",
        "        file_name = os.path.join(starting_directory, file)\n",
        "        if os.path.isdir(file_name):\n",
        "            final_list = final_list + get_file_list(file_name)\n",
        "        else:\n",
        "            final_list.append(file_name)\n",
        "    return final_list\n",
        "\n",
        "all_files = get_file_list()\n",
        "columns = [\n",
        "    'failure', \n",
        "    'capacity_bytes',\n",
        "    'smart_1_normalized',\n",
        "    'smart_2_normalized',\n",
        "    'smart_3_normalized',\n",
        "    'smart_4_normalized',\n",
        "    'smart_5_normalized',\n",
        "    'smart_7_normalized',\n",
        "    'smart_8_normalized',\n",
        "    'smart_9_normalized',\n",
        "    'smart_10_normalized',\n",
        "    'smart_11_normalized',\n",
        "    'smart_12_normalized',\n",
        "    'smart_13_normalized',\n",
        "    'smart_15_normalized',\n",
        "    'smart_16_normalized',\n",
        "    'smart_17_normalized',\n",
        "    'smart_18_normalized',\n",
        "    'smart_22_normalized',\n",
        "    'smart_23_normalized',\n",
        "    'smart_24_normalized',\n",
        "    'smart_168_normalized',\n",
        "    'smart_170_normalized',\n",
        "    'smart_173_normalized',\n",
        "    'smart_174_normalized',\n",
        "    'smart_175_normalized',\n",
        "    'smart_177_normalized',\n",
        "    'smart_179_normalized',\n",
        "    'smart_180_normalized',\n",
        "    'smart_181_normalized',\n",
        "    'smart_182_normalized',\n",
        "    'smart_183_normalized',\n",
        "    'smart_184_normalized',\n",
        "    'smart_187_normalized',\n",
        "    'smart_188_normalized',\n",
        "    'smart_189_normalized',\n",
        "    'smart_190_normalized',\n",
        "    'smart_191_normalized',\n",
        "    'smart_192_normalized',\n",
        "    'smart_193_normalized',\n",
        "    'smart_194_normalized',\n",
        "    'smart_195_normalized',\n",
        "    'smart_196_normalized',\n",
        "    'smart_197_normalized',\n",
        "    'smart_198_normalized',\n",
        "    'smart_199_normalized',\n",
        "    'smart_200_normalized',\n",
        "    'smart_201_normalized',\n",
        "    'smart_202_normalized',\n",
        "    'smart_206_normalized',\n",
        "    'smart_210_normalized',\n",
        "    'smart_218_normalized',\n",
        "    'smart_220_normalized',\n",
        "    'smart_222_normalized',\n",
        "    'smart_223_normalized',\n",
        "    'smart_224_normalized',\n",
        "    'smart_225_normalized',\n",
        "    'smart_226_normalized',\n",
        "    'smart_231_normalized',\n",
        "    'smart_232_normalized',\n",
        "    'smart_233_normalized',\n",
        "    'smart_234_normalized',\n",
        "    'smart_235_normalized',\n",
        "    'smart_240_normalized',\n",
        "    'smart_241_normalized',\n",
        "    'smart_242_normalized',\n",
        "    'smart_245_normalized',\n",
        "    'smart_247_normalized',\n",
        "    'smart_248_normalized',\n",
        "    'smart_250_normalized',\n",
        "    'smart_251_normalized',\n",
        "    'smart_252_normalized',\n",
        "    'smart_254_normalized',\n",
        "    'smart_255_normalized'\n",
        "]\n",
        "started = False\n",
        "for f in all_files[:20]:\n",
        "    if started:\n",
        "        new_df = pd.read_csv(f, usecols=columns)\n",
        "        df = df.append(new_df, ignore_index=True)\n",
        "    else:\n",
        "        df = pd.read_csv(f, usecols=columns)\n",
        "        started = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY5sM5pD0LnP"
      },
      "source": [
        "Before we go further, we learned a hard lesson in the last lab.  Specifically, this method can take a *lot* of time to train.  Especially since we want to try several different SVMs, let's first do some dimensionality reduction.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 5.2\n",
        "\n",
        "Use the next cell to extract the labels column from the dataframe, saving it in an array and deleting it from the dataframe.  Once this is done, convert the data in the dataframe to a Numpy array.\n",
        "\n",
        "When you have completed this, once again create a set of training data and testing data.  To do so, simply split off 25% of the data as testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haY_46_20LnP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9OKeaa10LnP"
      },
      "source": [
        "With our reduced data in and and split into test and training data, let's try to train several different SVMs and measure their accuracy.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 5.3\n",
        "\n",
        "Using Scikit Learn, create a Support Vector Classifier that uses the `polynomial` kernel with a `degree` setting of 2.  Fit the model using the elements in the range 60000 to 70000.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBO8-yRz0LnQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnM7fGsB0LnQ"
      },
      "source": [
        "# <img src=\"../images/task.png\" width=20 height=20> Task 5.4\n",
        "\n",
        "Implement a function `accuracy()` that accepts two parameters:\n",
        "\n",
        "* Ground truth labels\n",
        "* Predicted labels\n",
        "\n",
        "The function should output an accuracy percentage.  Use this function to measure the accuracy of the SVM that you just trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKRFDIT10LnQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzzP6Dv_0LnQ"
      },
      "source": [
        "# <img src=\"../images/task.png\" width=20 height=20> Task 5.5\n",
        "\n",
        "Create another SVC that uses a polynomial of degree 5.  How does its accuracy compare to that of the 2nd degree classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNkL-QmX0LnQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69PXmHOF0LnR"
      },
      "source": [
        "# <img src=\"../images/task.png\" width=20 height=20> Task 5.6\n",
        "\n",
        "Create another SVM, this time using a Radial Basis Function as the kernel.  This will require that you define a value for $\\gamma$.  Iteratively try values from 0.4 to 0.8, checking the accuracy for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m430lAzi0LnR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjngVte80LnR"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "These results are *very* interesting!  The training data that we are using is very skewed.  There are only a handful of failed drives in amongst drives that have not failed.  Why, then, do these classifiers perform so much  better than the linear classifier?\n",
        "\n",
        "The simple reason is that there is no good way to create a linear boundary that separates the data.  Could more training samples that include failed drives help?  Perhaps, but when we have data that isn't linearly separable, it doesn't matter how many samples we provide, there will never be an effective classifier.\n",
        "\n",
        "Analyzing that data in higher dimensions provides a clever trick that allows the system to build a classification function."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}