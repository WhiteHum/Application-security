{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteHum/Application-security/blob/main/3_04_Support_Vector_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "YQoHdKtEAD6T"
      },
      "source": [
        "\\pagenumbering{gobble}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiRp_ULwAD6V"
      },
      "source": [
        "# Support Vector Classifiers\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this lab, we will begin our supervised learning journey with a consideration of Support Vector Classifiers.  Our end-goal in this series of labs is to examine supervised learning tools that can be used as predictors or classifiers.\n",
        "\n",
        "## Goals\n",
        "\n",
        "This lab will focus only on Support Vector Classifiers.  The end-state of this lab leads directly into the next lab, which expands on this topic to Support Vector Machines\n",
        "\n",
        "* Understand how to build a Support Vector Classifier\n",
        "\n",
        " \n",
        "## Estimated Time: 30 minutes\n",
        "\n",
        "This lab and the three that follow it will all make use of the same BackBlaze dataset that we examined yesterday.  After getting set up to read that data, we will decide which fields are of interest and load that entire dataset into memory for our analysis.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 4.1\n",
        "\n",
        "Please revist lab 2.1.  Extract the portions of that notebook that are necessary to read the BackBlaze dataset into a dataframe.  Using this as a basis, load all of the data ***from the first twenty files*** preserving only the failure status, capacity, and normalized smart values from the data.\n",
        "\n",
        "*Note:* You may wish to research the `usecols` keyword argument for `pandas.read_csv()`.  You may also wish to examine the `ignore_index` keyword argument and the `append()` convenience function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5cXV98vAD6W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_file_list(starting_directory=\"../data/data_Q4_2020/\"):\n",
        "    final_list = list()\n",
        "    files = os.listdir(starting_directory)\n",
        "    for file in files:\n",
        "        file_name = os.path.join(starting_directory, file)\n",
        "        if os.path.isdir(file_name):\n",
        "            final_list = final_list + get_file_list(file_name)\n",
        "        else:\n",
        "            final_list.append(file_name)\n",
        "    return final_list\n",
        "\n",
        "all_files = get_file_list()\n",
        "columns = [\n",
        "    'failure', \n",
        "    'capacity_bytes',\n",
        "    'smart_1_normalized',\n",
        "    'smart_2_normalized',\n",
        "    'smart_3_normalized',\n",
        "    'smart_4_normalized',\n",
        "    'smart_5_normalized',\n",
        "    'smart_7_normalized',\n",
        "    'smart_8_normalized',\n",
        "    'smart_9_normalized',\n",
        "    'smart_10_normalized',\n",
        "    'smart_11_normalized',\n",
        "    'smart_12_normalized',\n",
        "    'smart_13_normalized',\n",
        "    'smart_15_normalized',\n",
        "    'smart_16_normalized',\n",
        "    'smart_17_normalized',\n",
        "    'smart_18_normalized',\n",
        "    'smart_22_normalized',\n",
        "    'smart_23_normalized',\n",
        "    'smart_24_normalized',\n",
        "    'smart_168_normalized',\n",
        "    'smart_170_normalized',\n",
        "    'smart_173_normalized',\n",
        "    'smart_174_normalized',\n",
        "    'smart_175_normalized',\n",
        "    'smart_177_normalized',\n",
        "    'smart_179_normalized',\n",
        "    'smart_180_normalized',\n",
        "    'smart_181_normalized',\n",
        "    'smart_182_normalized',\n",
        "    'smart_183_normalized',\n",
        "    'smart_184_normalized',\n",
        "    'smart_187_normalized',\n",
        "    'smart_188_normalized',\n",
        "    'smart_189_normalized',\n",
        "    'smart_190_normalized',\n",
        "    'smart_191_normalized',\n",
        "    'smart_192_normalized',\n",
        "    'smart_193_normalized',\n",
        "    'smart_194_normalized',\n",
        "    'smart_195_normalized',\n",
        "    'smart_196_normalized',\n",
        "    'smart_197_normalized',\n",
        "    'smart_198_normalized',\n",
        "    'smart_199_normalized',\n",
        "    'smart_200_normalized',\n",
        "    'smart_201_normalized',\n",
        "    'smart_202_normalized',\n",
        "    'smart_206_normalized',\n",
        "    'smart_210_normalized',\n",
        "    'smart_218_normalized',\n",
        "    'smart_220_normalized',\n",
        "    'smart_222_normalized',\n",
        "    'smart_223_normalized',\n",
        "    'smart_224_normalized',\n",
        "    'smart_225_normalized',\n",
        "    'smart_226_normalized',\n",
        "    'smart_231_normalized',\n",
        "    'smart_232_normalized',\n",
        "    'smart_233_normalized',\n",
        "    'smart_234_normalized',\n",
        "    'smart_235_normalized',\n",
        "    'smart_240_normalized',\n",
        "    'smart_241_normalized',\n",
        "    'smart_242_normalized',\n",
        "    'smart_245_normalized',\n",
        "    'smart_247_normalized',\n",
        "    'smart_248_normalized',\n",
        "    'smart_250_normalized',\n",
        "    'smart_251_normalized',\n",
        "    'smart_252_normalized',\n",
        "    'smart_254_normalized',\n",
        "    'smart_255_normalized'\n",
        "]\n",
        "# write the code below \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFa4LyBmAD6X"
      },
      "source": [
        "Now that we have the data loaded, we'd like to try to work out a way to potentially predict drive failures.  Our idea is that we might be able to find some boundary condition that can be defined which makes the failed/not-failed drives linearly separable.\n",
        "\n",
        "To accomplish this, we need training data, which we now have in abundance.  Our training data really isn't ideal since there are only just over 400 rows in total that indicate that a drive has failed.\n",
        "\n",
        "In order to be able to evaluate how well our classification works, we'd like to have some ground truth data to test it out with.  To do this, let's isolate several rows that contain failed drives and an equal number of rows with drives that have not failed.  We will set these aside, deleting them from the dataframe.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 4.2\n",
        "\n",
        "Select 10 rows containing failed drives and 10 rows containing drives that have not failed.  Combine this data into a new testing dataframe and delete those rows from the master dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7ciYSpxAD6X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x9l_AM8AD6X"
      },
      "source": [
        "We're now ready to try to build a Support Vector Classifier.  To do so, we need to load support for SVCs from Scikit Learn.  To do so, we need to convert our dataframe into a Numpy array and reorganize our data slightly.\n",
        "\n",
        "It is traditional in statistical and machine learning to use X to represent the training data and Y to represent the training labels or expected outputs.  Currently, our labels (whether or not the drive has failed) are embedded in our training data.  We need to pull those apart.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 4.3\n",
        "\n",
        "Use the cell that follows to import `SVC` from `sklearn.svm`.  Additionally, convert the training data in our dataframe to the expected X and Y arrays, isolating the labels from the data.  Along the way, we need to eliminate any `NaN` values.\n",
        "\n",
        "Take this opportunity to process the testing dataframe in the same way, creating `x_test` and `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqcjWWbwAD6Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdOTtYMRAD6Y"
      },
      "source": [
        "It's time to train our classifier.  To do so, we need to instantiate an `SVC` object and fit on it:\n",
        "\n",
        "```\n",
        "svc = SVC(kernel='linear')\n",
        "svc.fit(x, y)\n",
        "```\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 4.4\n",
        "\n",
        "Use the following cell to create and train a linear Support Vector Classifier.  Since these can take a *very* long time to train, limit the model to train on data in the range 60000 to 70000. (The linear classifier can take a *very long time* to train... If you are impatient or pressed for time, consider reducing this to a much smaller range.  Just ensure that your training sample includes both failed and non-failed drives!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWMccWC4AD6Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax40MCkKAD6Y"
      },
      "source": [
        "We'd like to evaluate the classifier to see how well it does.  Honestly, we aren't expecting a linear classifier to do very well at all, so we won't get our hopes up...\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 4.5\n",
        "\n",
        "Use the `predict()` method on your SVC model, passing it the `x_test` data.  The returned array is the classification predictions for that data.  Compare the predicted labels to the known labels in `y_test` to determine the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcYggD5oAD6Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcOgrU0LAD6Z"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "There is more to say about Support Vector Classifiers.  We will continue this discussion in the next lab, picking up where we are leaving off here.  Still, there are some important takeaways.\n",
        "\n",
        "We again find that preprocessing the data is more time consuming and attention intensive the creating the model.  We also discover that, as simple as these classifiers are, the training time increases directly as a result of the number of samples that we are processing.\n",
        "\n",
        "While this is intuitive and may seem obvious, you will find that other types of models have a very different set of training characteristics.  While we always expect more data to take more time to train on, is the increase in time exponential?  Is it linear?  Is it logarithmic?  Linear or worse can have a very big impact on our ability to repeatedly train a model.  In the context of Support Vectors this is important because the general practice is to perform a grid search to determine parameter selection for the classifier.  This requires that we retrain the model with all possible combinations that we include in our search, which can be very time consuming.\n",
        "\n",
        "Why is this particular example so slow?  Think about what is required.  The classifier must calculate the distance of every point from every other point and work out a decision boundary that separates the data.  If we have 100,000 with 73 dimensions each, that adds up to a *lot* of processing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-f3ffJsAD6Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}