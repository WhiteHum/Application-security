{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteHum/Application-security/blob/main/3_07_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "0bvWXEF3GLac"
      },
      "source": [
        "\\pagenumbering{gobble}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lekJ82wGLae"
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "## Overview\n",
        "\n",
        "We saw how easy it was to create a Decision Tree, but we recognize that the accuracy may suffer when looking at new data.  Random Forest is an approach that seeks to improve this by building many trees using Bagging and a measure of randomization.\n",
        "\n",
        "## Goals\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Build and train a Random Forest classifier\n",
        "* Compare different parameter options when building the classifier\n",
        "* Evaluate the accuracy in comparison to a Decision Tree\n",
        "\n",
        " \n",
        "## Estimated Time: 30 minutes\n",
        "\n",
        "We will once again be using the same BackBlaze data in this lab.  Hopefully, this has given us an opportunity to compare apples to apples across these four labs.\n",
        "\n",
        "## Note Regarding This Lab\n",
        "\n",
        "Our decision tree performs so well, that we aren't going to see any real benefit from switching to a Random Forest.  We have incldued a completely different problem at the end of this lab to demonstrate how a random forest can perform better than a simple decision tree.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 7.1\n",
        "\n",
        "Use the following cell to read in the first twenty files from the BackBlaze dataset, split the labels from the data, convert everything to Numpy arrays, and create a test dataset from the final 25% of the overall dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5Ns1g7vGLaf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_file_list(starting_directory=\"../data/data_Q4_2020/\"):\n",
        "    final_list = list()\n",
        "    files = os.listdir(starting_directory)\n",
        "    for file in files:\n",
        "        file_name = os.path.join(starting_directory, file)\n",
        "        if os.path.isdir(file_name):\n",
        "            final_list = final_list + get_file_list(file_name)\n",
        "        else:\n",
        "            final_list.append(file_name)\n",
        "    return final_list\n",
        "\n",
        "all_files = get_file_list()\n",
        "columns = [\n",
        "    'failure', \n",
        "    'capacity_bytes',\n",
        "    'smart_1_normalized',\n",
        "    'smart_2_normalized',\n",
        "    'smart_3_normalized',\n",
        "    'smart_4_normalized',\n",
        "    'smart_5_normalized',\n",
        "    'smart_7_normalized',\n",
        "    'smart_8_normalized',\n",
        "    'smart_9_normalized',\n",
        "    'smart_10_normalized',\n",
        "    'smart_11_normalized',\n",
        "    'smart_12_normalized',\n",
        "    'smart_13_normalized',\n",
        "    'smart_15_normalized',\n",
        "    'smart_16_normalized',\n",
        "    'smart_17_normalized',\n",
        "    'smart_18_normalized',\n",
        "    'smart_22_normalized',\n",
        "    'smart_23_normalized',\n",
        "    'smart_24_normalized',\n",
        "    'smart_168_normalized',\n",
        "    'smart_170_normalized',\n",
        "    'smart_173_normalized',\n",
        "    'smart_174_normalized',\n",
        "    'smart_175_normalized',\n",
        "    'smart_177_normalized',\n",
        "    'smart_179_normalized',\n",
        "    'smart_180_normalized',\n",
        "    'smart_181_normalized',\n",
        "    'smart_182_normalized',\n",
        "    'smart_183_normalized',\n",
        "    'smart_184_normalized',\n",
        "    'smart_187_normalized',\n",
        "    'smart_188_normalized',\n",
        "    'smart_189_normalized',\n",
        "    'smart_190_normalized',\n",
        "    'smart_191_normalized',\n",
        "    'smart_192_normalized',\n",
        "    'smart_193_normalized',\n",
        "    'smart_194_normalized',\n",
        "    'smart_195_normalized',\n",
        "    'smart_196_normalized',\n",
        "    'smart_197_normalized',\n",
        "    'smart_198_normalized',\n",
        "    'smart_199_normalized',\n",
        "    'smart_200_normalized',\n",
        "    'smart_201_normalized',\n",
        "    'smart_202_normalized',\n",
        "    'smart_206_normalized',\n",
        "    'smart_210_normalized',\n",
        "    'smart_218_normalized',\n",
        "    'smart_220_normalized',\n",
        "    'smart_222_normalized',\n",
        "    'smart_223_normalized',\n",
        "    'smart_224_normalized',\n",
        "    'smart_225_normalized',\n",
        "    'smart_226_normalized',\n",
        "    'smart_231_normalized',\n",
        "    'smart_232_normalized',\n",
        "    'smart_233_normalized',\n",
        "    'smart_234_normalized',\n",
        "    'smart_235_normalized',\n",
        "    'smart_240_normalized',\n",
        "    'smart_241_normalized',\n",
        "    'smart_242_normalized',\n",
        "    'smart_245_normalized',\n",
        "    'smart_247_normalized',\n",
        "    'smart_248_normalized',\n",
        "    'smart_250_normalized',\n",
        "    'smart_251_normalized',\n",
        "    'smart_252_normalized',\n",
        "    'smart_254_normalized',\n",
        "    'smart_255_normalized'\n",
        "]\n",
        "started = False\n",
        "for f in all_files[:20]:\n",
        "    if started:\n",
        "        new_df = pd.read_csv(f, usecols=columns)\n",
        "        df = df.append(new_df, ignore_index=True)\n",
        "    else:\n",
        "        df = pd.read_csv(f, usecols=columns)\n",
        "        started = True\n",
        "\n",
        "# Grab the labels\n",
        "labels = df['failure'].to_numpy()\n",
        "# Drop the failure column\n",
        "df.drop('failure', axis=1, inplace=True)\n",
        "\n",
        "# Convert to an array and replace NaN values\n",
        "x = df.to_numpy()\n",
        "for i in np.argwhere(np.isnan(x)):\n",
        "    x[i[0],i[1]] = 0\n",
        "\n",
        "testing_split = int(len(labels)*0.25)\n",
        "\n",
        "x_train = x[:testing_split]\n",
        "y_train = labels[:testing_split]\n",
        "x_test = x[testing_split:]\n",
        "y_test = labels[testing_split:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wnlrmKqGLag"
      },
      "source": [
        "The Random Forest approach has been implemented in Scikit Learn within the `sklearn.ensemble` package.  Let's get started by importing it.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 7.2\n",
        "\n",
        "Import the `RandomForestClassifier` from Scikit Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvFE1-3CGLag"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56nQ_3VGLag"
      },
      "source": [
        "To build a Random Forest, there are several parameters that we can tune to speed up the process and possibly increase our accuracy:\n",
        "\n",
        "* The number of *estimators* or trees that will be in the forest.  This is selected with the `n_estimators` parameter.\n",
        "* The maximum depth of each tree, which is controlled by the `max_depth` parameter.  If you do not set this parameter, every tree may grow to any size based on how well the data splits at each node.\n",
        "* We can specify the number of jobs, or `n_jobs`, which instructs the library to use some number of threads to build trees in parallel.  Setting this to `-1` it will use all available processors.  The default sets this to `1`.\n",
        "* The maximum number of samples to use when training each tree.  This parameter is `max_samples`.\n",
        "* The maximum number of features to consider when splitting the data.  The default is $\\sqrt{n}$ where $n$ is the number of features.  We can tune this value using `max_features`.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 7.3\n",
        "\n",
        "Create a Random Forest with 100 trees and a maximum depth of 20.  What is the accuracy against the test dataset?  When training this and all future forests in this lab, set `max_samples` to 30% of the total number of samples in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIXMrE6sGLag"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTZwFBNuGLah"
      },
      "source": [
        "One of the things mentioned in the discussion is that we have to balance overall accuracy against time.  One of the parameters that affects this most severely is the number of trees.  Let's train this again, this time with 1,000 trees.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 7.4\n",
        "\n",
        "Does a forest with 1,000 trees with a maximum depth of 20 have a significantly greater accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubqNkZn2GLah"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLBA02HBGLah"
      },
      "source": [
        "Something that will tend to speed up training is reducing the number of features considered at each node.  This will simultaneously tend to decrease the accuracy of each tree, which can be smoothed out by having more trees.\n",
        "\n",
        "# <img src=\"../images/task.png\" width=20 height=20> Task 7.5\n",
        "\n",
        "Construct and test two forests.  The first has 100 trees and the second 1,000.  For each of these, set the `max_features` parameter to 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sM11s_AGLah"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxkhOtqXGLah"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}